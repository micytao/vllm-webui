# vLLM Playground

A modern web interface for managing and interacting with vLLM servers (www.github.com/vllm-project/vllm). Supports both GPU and CPU modes, with special optimizations for macOS Apple Silicon.

![vLLM Playground Interface](assets/vllm-playground.png)

## ğŸ†• New: Model Compression Support

Built-in LLM-Compressor integration for quantizing and compressing models directly from the UI!

![Model Compression Interface](assets/llmcompressor.png)

## ğŸ“Š New: GuideLLM Benchmarking

Integrated GuideLLM for comprehensive performance benchmarking and analysis. Run load tests and get detailed metrics on throughput, latency, and token generation performance!

![GuideLLM Benchmark Results](assets/guidellm.png)

## ğŸ“ Project Structure

```
vllm-playground/
â”œâ”€â”€ app.py                       # Main FastAPI backend application
â”œâ”€â”€ run.py                       # Backend server launcher
â”œâ”€â”€ index.html                   # Main HTML interface
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ env.example                  # Example environment variables
â”œâ”€â”€ LICENSE                      # MIT License
â”œâ”€â”€ README.md                    # This file
â”‚
â”œâ”€â”€ containers/                  # Container definitions ğŸ³
â”‚   â”œâ”€â”€ Containerfile.cuda      # CUDA/GPU container (RHEL UBI9)
â”‚   â”œâ”€â”€ Containerfile.vllm      # vLLM official base image
â”‚   â”œâ”€â”€ Containerfile.mac       # macOS/CPU container
â”‚   â””â”€â”€ Containerfile.rhel9     # RHEL 9 container
â”‚
â”œâ”€â”€ deployments/                 # Kubernetes/OpenShift deployments â˜¸ï¸
â”‚   â”œâ”€â”€ kubernetes-deployment.yaml    # Kubernetes manifests
â”‚   â”œâ”€â”€ openshift-deployment.yaml     # OpenShift manifests
â”‚   â””â”€â”€ deploy-to-openshift.sh       # OpenShift deployment script
â”‚
â”œâ”€â”€ static/                      # Frontend assets
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css           # Main stylesheet
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ app.js              # Frontend JavaScript
â”‚
â”œâ”€â”€ scripts/                     # Utility scripts
â”‚   â”œâ”€â”€ run_cpu.sh              # Start vLLM in CPU mode (macOS compatible)
â”‚   â”œâ”€â”€ start.sh                # General start script
â”‚   â”œâ”€â”€ install.sh              # Installation script
â”‚   â””â”€â”€ verify_setup.py         # Setup verification
â”‚
â”œâ”€â”€ config/                      # Configuration files
â”‚   â”œâ”€â”€ vllm_cpu.env            # CPU mode environment variables
â”‚   â””â”€â”€ example_configs.json    # Example configurations
â”‚
â”œâ”€â”€ assets/                      # Images and assets
â”‚   â”œâ”€â”€ vllm-playground.png          # WebUI screenshot
â”‚   â”œâ”€â”€ llmcompressor.png       # Model compression UI screenshot
â”‚   â”œâ”€â”€ guidellm.png            # GuideLLM benchmark results screenshot
â”‚   â”œâ”€â”€ vllm.png                # vLLM logo
â”‚   â””â”€â”€ vllm.jpeg               # vLLM logo (alternate)
â”‚
â””â”€â”€ docs/                        # Documentation
    â”œâ”€â”€ QUICKSTART.md            # Quick start guide
    â”œâ”€â”€ MACOS_CPU_GUIDE.md       # macOS CPU setup guide
    â”œâ”€â”€ CPU_MODELS_QUICKSTART.md # CPU-optimized models guide
    â”œâ”€â”€ GATED_MODELS_GUIDE.md    # Guide for accessing Llama, Gemma, etc.
    â”œâ”€â”€ CHAT_TEMPLATES.md        # Model-specific chat templates
    â”œâ”€â”€ TROUBLESHOOTING.md       # Common issues and solutions
    â”œâ”€â”€ FEATURES.md              # Feature documentation
    â”œâ”€â”€ PERFORMANCE_METRICS.md   # Performance metrics
    â””â”€â”€ QUICK_REFERENCE.md       # Command reference
```

## ğŸš€ Quick Start

### ğŸ³ Option 1: Container (Easiest for macOS) **RECOMMENDED**

For macOS users, the container provides the easiest setup with everything pre-configured:

```bash
# 1. Build the container (one-time, ~15-30 min)
./scripts/build_container.sh

# 2. Run the container
./scripts/run_container.sh

# 3. Open http://localhost:7860
```

**âœ¨ Benefits:**
- âœ… No complex installation
- âœ… Pre-built vLLM optimized for CPU
- âœ… Isolated environment
- âœ… Works out of the box

**ğŸ“– See [CONTAINER-QUICKSTART.md](CONTAINER-QUICKSTART.md)** for detailed instructions.

---

### ğŸ’» Option 2: Local Installation

For local development or if you prefer not to use containers:

#### 1. Install vLLM

```bash
# For macOS/CPU mode
pip install vllm
```

#### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

#### 3. Start the WebUI

```bash
python run.py
```

Then open http://localhost:7860 in your browser.

#### 4. Start vLLM Server

**Option A: Using the WebUI**
- Select CPU or GPU mode
- Click "Start Server"

**Option B: Using the script (macOS/CPU)**
```bash
./scripts/run_cpu.sh
```

## ğŸ’» macOS Apple Silicon Support

For macOS users, vLLM runs in CPU mode. See [docs/MACOS_CPU_GUIDE.md](docs/MACOS_CPU_GUIDE.md) for detailed setup.

**Quick CPU Mode Setup:**
```bash
# Edit CPU configuration
nano config/vllm_cpu.env

# Run vLLM
./scripts/run_cpu.sh
```

## âœ¨ Features

- **Model Compression**: LLM-Compressor integration for quantizing and compressing models ğŸ†•
- **Performance Benchmarking**: GuideLLM integration for comprehensive load testing with detailed metrics ğŸ†•
  - Request statistics (success rate, duration, avg times)
  - Token throughput analysis (mean/median tokens per second)
  - Latency percentiles (P50, P75, P90, P95, P99)
  - Configurable load patterns and request rates
- **Server Management**: Start/stop vLLM servers from the UI
- **Chat Interface**: Interactive chat with streaming responses
- **Smart Chat Templates**: Automatic model-specific template detection (Nov 2025) ğŸ†•
- **Performance Metrics**: Real-time token counts and generation speed
- **Model Support**: Pre-configured popular models + custom model support
- **Gated Model Access**: Built-in HuggingFace token support for Llama, Gemma, etc.
- **CPU & GPU Modes**: Automatic detection and configuration
- **macOS Optimized**: Special support for Apple Silicon
- **Resizable Panels**: Customizable layout
- **Command Preview**: See exact commands before execution

## ğŸ“– Documentation

### Getting Started
- **[Container Quick Start](CONTAINER-QUICKSTART.md)** ğŸ³ - Easiest way for macOS users (RECOMMENDED)
- **[Container Full Guide](README-CONTAINER.md)** - Complete container documentation
- **[Container Workflow](CONTAINER-WORKFLOW.md)** - Step-by-step container workflow
- **[Quick Start Guide](docs/QUICKSTART.md)** - Get up and running in minutes
- **[Command-Line Demo Guide](cli_demo/docs/CLI_DEMO_GUIDE.md)** ğŸ†• - Full workflow demo with vLLM, LLMCompressor & GuideLLM
- [macOS CPU Setup](docs/MACOS_CPU_GUIDE.md) - Apple Silicon optimization guide
- [CPU Models Quickstart](docs/CPU_MODELS_QUICKSTART.md) - Best models for CPU

### Model Configuration
- **[Gated Models Guide (Llama, Gemma)](docs/GATED_MODELS_GUIDE.md)** â­ - Access restricted models
- **[Chat Templates Explained](docs/CHAT_TEMPLATES.md)** ğŸ†• - Model-specific templates

### Reference
- [Feature Overview](docs/FEATURES.md) - Complete feature list
- [Performance Metrics](docs/PERFORMANCE_METRICS.md) - Benchmarking and metrics
- [Command Reference](docs/QUICK_REFERENCE.md) - Command cheat sheet
- [CLI Quick Reference](cli_demo/docs/CLI_QUICK_REFERENCE.md) - Command-line demo quick reference ğŸ†•
- [Troubleshooting](docs/TROUBLESHOOTING.md) - Common issues and solutions

## ğŸ”§ Configuration

### CPU Mode (macOS)

Edit `config/vllm_cpu.env`:
```bash
export VLLM_CPU_KVCACHE_SPACE=40
export VLLM_CPU_OMP_THREADS_BIND=auto
```

### Supported Models

**CPU-Optimized Models (Recommended for macOS):**
- **TinyLlama/TinyLlama-1.1B-Chat-v1.0** (default) - Fast, no token required
- **meta-llama/Llama-3.2-1B** - Latest Llama, requires HF token (gated)
- **google/gemma-2-2b** - High quality, requires HF token (gated)
- facebook/opt-125m - Tiny test model

**Larger Models (Slow on CPU, better on GPU):**
- meta-llama/Llama-2-7b-chat-hf (requires HF token)
- mistralai/Mistral-7B-Instruct-v0.2
- Custom models via text input

**ğŸ“Œ Note**: Gated models (Llama, Gemma) require a HuggingFace token. See [Gated Models Guide](docs/GATED_MODELS_GUIDE.md) for setup.

## ğŸ› ï¸ Development

### Project Structure

- **Backend**: FastAPI (`app.py`)
- **Frontend**: Vanilla JavaScript (`static/js/app.js`)
- **Styling**: Custom CSS (`static/css/style.css`)
- **Scripts**: Bash scripts in `scripts/`
- **Config**: Environment files in `config/`

### Running in Development

```bash
# Start backend with auto-reload
uvicorn app:app --reload --port 7860

# Or use the run script
python run.py
```

## ğŸ“ License

MIT License - See [LICENSE](LICENSE) file for details

## ğŸ¤ Contributing

Contributions welcome! Please feel free to submit issues and pull requests.

## ğŸ”— Links

- [vLLM Official Documentation](https://docs.vllm.ai/)
- [vLLM CPU Mode Guide](https://docs.vllm.ai/en/stable/getting_started/installation/cpu.html)
- [vLLM GitHub](https://github.com/vllm-project/vllm)

## ğŸ†˜ Troubleshooting

### macOS Segmentation Fault

Use CPU mode with proper environment variables. See [docs/MACOS_CPU_GUIDE.md](docs/MACOS_CPU_GUIDE.md).

### Server Won't Start

1. Check if vLLM is installed: `python -c "import vllm; print(vllm.__version__)"`
2. Check port availability: `lsof -i :8000`
3. Review server logs in the WebUI

### Chat Not Streaming

Check browser console (F12) for errors and ensure the server is running.

---

Made with â¤ï¸ for the vLLM community
